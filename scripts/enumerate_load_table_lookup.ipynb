{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import os\n",
    "import math\n",
    "import shutil, errno\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark(name, mem=\"5gb\", num_cpus=None):\n",
    "    \"\"\" Initialize a SparkSession. \"\"\"\n",
    "    if num_cpus is None:\n",
    "        num_cpus = multiprocessing.cpu_count()\n",
    " \n",
    "    return SparkSession.Builder() \\\n",
    "        .master('local') \\\n",
    "        .appName(name) \\\n",
    "        .config('spark.executor.memory', mem) \\\n",
    "        .config(\"spark.cores.max\", str(num_cpus)) \\\n",
    "        .config(\"spark.logConf\", \"true\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copydirectory(src, dst, override=False):\n",
    "    \"\"\" Copy directory from src to dst, with option to override. \"\"\"\n",
    "    if (override == False) & (os.path.exists(dst)):\n",
    "        print(f'\"{dst}\" already exists, passing...')\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        if os.path.exists(dst):\n",
    "            shutil.rmtree(dst)\n",
    "        try:\n",
    "            shutil.copytree(src, dst)\n",
    "        except OSError as exc: # python >2.5\n",
    "            if exc.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "                shutil.copy(src, dst)\n",
    "            else: raise\n",
    "        print(f'\"{src}\"\\n copied to: \"{dst}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark('dsgrid-load') # <--- same as a pyspark instance in shell\n",
    "lookup_file = '/Users/lliu2/Documents/dsGrid/dsgrid_v2.0.0/commercial/load_data_lookup.parquet' # <---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### relocation the original load_data_lookup\n",
    "# define path for relocation\n",
    "file_rename = list(os.path.splitext(\n",
    "    os.path.basename(lookup_file)\n",
    "))\n",
    "file_rename[0] = file_rename[0]+'_orig'\n",
    "file_rename = ''.join(file_rename)\n",
    "relocated_file = os.path.join(\n",
    "    os.path.dirname(lookup_file),\n",
    "    file_rename\n",
    ")\n",
    "\n",
    "# execute\n",
    "copydirectory(lookup_file, relocated_file, override=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data as a Spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data from relocated file\n",
    "df_lookup = spark.read.parquet(relocated_file)\n",
    "    \n",
    "# df_lookup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get keys to enumerate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_exclude = ['scale_factor', 'data_id', 'id']\n",
    "keys = [x for x in df_lookup.columns if x not in keys_to_exclude]\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Expand Load_data_lookup to all combinations of keys, keep new combinations null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(keys)>1:\n",
    "    df_lookup_full = df_lookup.select(keys[0]).distinct()\n",
    "\n",
    "    for key in keys[1:]:\n",
    "        df_lookup_full = df_lookup_full.crossJoin(\n",
    "            df_lookup.select(key).distinct()\n",
    "        )\n",
    "\n",
    "    df_lookup_full = df_lookup_full.join(\n",
    "        df_lookup, keys, 'left'\n",
    "    ).sort(['id']+keys)\n",
    "else:\n",
    "    df_lookup_full = df_lookup\n",
    "    \n",
    "# df_lookup_full.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Check\n",
    "#### 4.1. Mapping Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_df_lookup = df_lookup.count()\n",
    "N_df_lookup_full = df_lookup_full.count()\n",
    "N_df_lookup_null = N_df_lookup_full - N_df_lookup\n",
    "print(f'# rows in df_lookup: {N_df_lookup}')\n",
    "print(f'# rows in df_lookup (fully enumerated): {N_df_lookup_full}')\n",
    "print(f'# of rows without data: {N_df_lookup_null} ({(N_df_lookup_null/N_df_lookup_full*100):.02f}%)')\n",
    "\n",
    "# df_lookup_full.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Assertion checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) make sure N_df_lookup_full is the product of the length of each key\n",
    "N_enumerations = 1\n",
    "for key in keys:\n",
    "    N_enumerations *= df_lookup.select(key).distinct().count()\n",
    "    \n",
    "assert N_df_lookup_full == N_enumerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) number of id list is the same before and after enumeration\n",
    "df_lookup_ids = df_lookup.select('id').distinct().toPandas().iloc[:, 0].values\n",
    "df_lookup_full_ids = df_lookup_full.select('id').distinct().toPandas().iloc[:, 0].values\n",
    "\n",
    "assert len(set(df_lookup_ids).difference(df_lookup_full_ids)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_size(df, bytes_per_cell=64):\n",
    "    \"\"\" approximate dataset size \"\"\"\n",
    "    n_rows = df.count()\n",
    "    n_cols = len(df.columns)\n",
    "    data_MB = n_rows*n_cols*bytes_per_cell / 1e6  # MiB\n",
    "    \n",
    "    return n_rows, n_cols, data_MB\n",
    "\n",
    "def get_optimal_number_of_files(df, MB_per_file=128):\n",
    "    \"\"\" calculate *optimal* number of files \"\"\"\n",
    "    _, _, data_MB = get_data_size(df)\n",
    "    n_files = math.ceil(data_MB / MB_per_file)\n",
    "\n",
    "    print(\n",
    "        f\"load_data_lookup is approximately {data_MB:.02f} MB in size, ideal to split into {n_files} file(s) at 128 MB each.\"\n",
    "    )\n",
    "\n",
    "    return n_files\n",
    "\n",
    "def file_size_if_partition_by(df, key):\n",
    "    n_rows, n_cols, data_MB = get_data_size(df)\n",
    "    n_partitions = df.select(key).distinct().count()\n",
    "    avg_MB = round(data_MB / n_partitions, 2)\n",
    "    \n",
    "    n_rows_largest_part = df.groupBy(key).count().orderBy('count', ascending=False).first()[1]\n",
    "    n_rows_smallest_part = df.groupBy(key).count().orderBy('count', ascending=True).first()[1]\n",
    "    \n",
    "    largest_MB = round(data_MB / n_rows * n_rows_largest_part, 2)\n",
    "    smallest_MB = round(data_MB / n_rows * n_rows_smallest_part, 2)\n",
    "    \n",
    "    report = (\n",
    "        f'Partitioning by \"{key}\" will yield: \\n' +\n",
    "        f'  - # of partitions: {n_partitions} \\n' +\n",
    "        f'  - avg partition size: {avg_MB} MB \\n' +\n",
    "        f'  - largest partition: {largest_MB} MB \\n' +\n",
    "        f'  - smallest partition: {smallest_MB} MB \\n'\n",
    "    )\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    output = pd.DataFrame(\n",
    "        {key: [n_partitions, avg_MB, largest_MB, smallest_MB]},\n",
    "        index=['n_partitions', 'avg_partition_MB', 'max_partition_MB', 'min_partition_MB']\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. check partitioning choices and *optimal* # of sharded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = df_lookup_full.columns\n",
    "\n",
    "partition_stats = []\n",
    "for key in df_cols:\n",
    "    report = file_size_if_partition_by(df_lookup_full, key)\n",
    "    partition_stats.append(report)\n",
    "\n",
    "partition_stats = pd.concat(partition_stats, axis=1)\n",
    "\n",
    "partition_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *optimal* # of files\n",
    "n_files = get_optimal_number_of_files(df_lookup_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, filepath, n_files=None, repartition_by=None):\n",
    "    \"\"\" \n",
    "    n_files: number of target sharded files\n",
    "    repartition_by: col to repartition by\n",
    "    \n",
    "    Note: \n",
    "        - Not available for load_data_lookup: df.write.partitionBy().bucketBy()\n",
    "        - df.coalesce(n): combine without shuffling, will not go larger than current_n_files\n",
    "        - df.repartition(n): shuffle and try to evenly distribute, if n > # of unique rows, some partitions will be empty\n",
    "        - df.repartition(col): shuffle and create partitions by unique record in col + 1 empty/very small partition\n",
    "        - df.repartition(n, col): shufffle, number partitions = min(n, unique record in col)\n",
    "    \"\"\"\n",
    "    \n",
    "    current_n_parts = df.rdd.getNumPartitions()\n",
    "    \n",
    "    if n_files != None and repartition_by != None:\n",
    "        df_out = df.repartition(n_files, repartition_by)\n",
    "    elif n_files == None and repartition_by != None:\n",
    "        df_out = df.repartition(repartition_by) \n",
    "    elif n_files != None and repartition_by == None:\n",
    "        df_out = df.repartition(n_files) \n",
    "    else:\n",
    "        df_out = df\n",
    "    \n",
    "    # for reporting out:\n",
    "    if repartition_by != None:\n",
    "        n_out_files = df.select(repartition_by).distinct().count() + 1\n",
    "        ext = f', repartitioned by {repartition_by}'\n",
    "    else:\n",
    "        n_out_files = current_n_parts\n",
    "        ext = ''\n",
    "    n_out_files = min(n_out_files, df_out.rdd.getNumPartitions())\n",
    "    print(f'Saving {current_n_parts} partitions --> {n_out_files} file(s){ext}...')\n",
    "    \n",
    "    df_out.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"path\", filepath)\\\n",
    "        .saveAsTable(\"load_data_lookup\", format='parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- `write.partitionBy('col1','col2',...)`: export partitions by creating hierarchical subfolders (e.g., col1=0/col2=0/col3=.../part-0)\n",
    "- `write.option(\"maxRecordsPerFile\", n).partitionBy(col)`: use to control # of unique records (to n) per partition\n",
    "- `coalesce(n).write`: combine into n partitions without shuffling, will not go larger than # of RDD files (spark default is 200)\n",
    "- `repartition(n).write`: try to evenly distribute, if n > # of unique rows, some partitions will be empty\n",
    "- `repartition(col).write`: create partitions by unique col field, 1 empty/very small partition will be created in addition to # of unique col records\n",
    "- `repartition(n, col).write`: # files exported = min(n, # of unique fields for col)\n",
    "- `repartition(n).write.partitionBy(col)`: create subfolder by unique col fields, each subfolder contains n partitions\n",
    "- `write.partitionBy(col1).bucketBy(n_buckets, col2)`: distribute partitions into smaller pieces called buckets, col2 can not be the same as col1, good for reducing shuffles/exchanges when tables get joined, # of files exported = n_unique_fields_in_col1 x n_buckets x n_repartitions (if applicable)\n",
    "\n",
    "File format: part-[partiton#]-[bucket#]...snappy.parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "`df_lookup_full.repartition(3).write \\\n",
    "    .partitionBy(\"sector\") \\\n",
    "    .bucketBy(2, \"subsector\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", lookup_file)\\\n",
    "    .saveAsTable(\"load_data_lookup\", format='parquet')`\n",
    "    \n",
    "Outputs:\n",
    "\n",
    "```\n",
    "load_data_lookup.parquet\n",
    "├── _SUCCESS\n",
    "├── sector=com\n",
    "│   ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "│   ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "│   ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "│   ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "│   ├── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "│   └── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "└── sector=res\n",
    "    ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "    ├── part-00000-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "    ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "    ├── part-00001-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "    ├── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00000.c000.snappy.parquet\n",
    "    └── part-00002-4943b363-fbac-4665-8c76-d771c3f6cbbb_00001.c000.snappy.parquet\n",
    "\n",
    "2 directories (controlled by `partitionBy`), 13 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(df_lookup_full, lookup_file, n_files, repartition_by=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
