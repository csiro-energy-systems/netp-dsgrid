{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93935e00",
   "metadata": {},
   "source": [
    "## 1. initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c46803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T18:38:03.085935Z",
     "start_time": "2022-06-08T18:38:02.932167Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import getpass\n",
    "import shutil\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e730c",
   "metadata": {},
   "source": [
    "## 2. start spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fac86c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T18:38:03.813868Z",
     "start_time": "2022-06-08T18:38:03.809457Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweak setting here:\n",
    "def init_spark(cluster=None, name=\"dsgrid\", tz=\"UTC\"):\n",
    "    \"\"\"Initialize a SparkSession.\"\"\"\n",
    "    if cluster is None:\n",
    "        spark = SparkSession.builder.master(\"local\").appName(name).getOrCreate()\n",
    "    else:\n",
    "        conf = SparkConf().setAppName(name).setMaster(cluster)\n",
    "        conf = conf.setAll([\n",
    "    #         (\"spark.sql.shuffle.partitions\", \"200\"),\n",
    "    #         (\"spark.executor.instances\", \"7\"),\n",
    "    #         (\"spark.executor.cores\", \"5\"),\n",
    "    #         (\"spark.executor.memory\", \"10g\"),\n",
    "    #         (\"spark.driver.memory\", \"10g\"),\n",
    "#             (\"spark.dynamicAllocation.enabled\", True),\n",
    "#             (\"spark.shuffle.service.enabled\", True),\n",
    "            (\"spark.sql.session.timeZone\", tz),\n",
    "        ])\n",
    "        spark = (\n",
    "                SparkSession.builder.config(conf=conf)\n",
    "                .getOrCreate()\n",
    "            )\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0e446",
   "metadata": {},
   "source": [
    "To launch a standalone cluster or a cluster on Eagle, follow **instructions** here: \\\n",
    "https://github.com/dsgrid/dsgrid/tree/main/dev#spark-standalone-cluster\n",
    "\n",
    "accordingly, uncomment and update the cluster name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3c099a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T18:39:08.247437Z",
     "start_time": "2022-06-08T18:38:05.558136Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/08 18:38:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/08 18:38:08 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 172.18.27.8:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: Failed to connect to /172.18.27.8:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.18.27.8:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/06/08 18:38:28 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 172.18.27.8:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: Failed to connect to /172.18.27.8:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.18.27.8:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/06/08 18:38:48 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 172.18.27.8:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: Failed to connect to /172.18.27.8:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /172.18.27.8:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/08 18:39:08 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "22/06/08 18:39:08 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "22/06/08 18:39:08 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n",
      "22/06/08 18:39:08 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16782/237816582.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dsgrid-load'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_tz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# get Spark Context UI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16782/1000262140.py\u001b[0m in \u001b[0;36minit_spark\u001b[0;34m(cluster, name, tz)\u001b[0m\n\u001b[1;32m     17\u001b[0m         ])\n\u001b[1;32m     18\u001b[0m         spark = (\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 147\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1574\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "main_tz = \"EST\" # <--- UTC, EST\n",
    "\n",
    "\n",
    "### STAND-ALONE CLUSTER\n",
    "# cluster = \"spark://lliu2-34727s:7077\" \n",
    "\n",
    "### CLUSTER ON HPC - Type in nodename\n",
    "# NODENAME = \"r103u23\" # <--- change after deploying cluster\n",
    "# cluster = f\"spark://{NODENAME}.ib0.cm.hpc.nrel.gov:7077\" \n",
    "\n",
    "# ETH@Review: Adding this section of code because I started the notebook \n",
    "# up per the instructions I made before\n",
    "### CLUSTER ON HPC - Get cluster from file dropped by prep_spark_cluster_notebook.py\n",
    "# import toml\n",
    "# config = toml.load(\"cluster.toml\")\n",
    "# cluster = config[\"cluster\"]\n",
    "\n",
    "### LOCAL MODE\n",
    "# cluster = None\n",
    "\n",
    "### AWS EMR cluster MODE\n",
    "NODENAME = \"172.18.27.8\" #\"ec2-54-212-53-141.us-west-2.compute.amazonaws.com\"\n",
    "cluster = f\"spark://{NODENAME}:7077\" \n",
    "\n",
    "# Initialize\n",
    "spark = init_spark(cluster, 'dsgrid-load', tz=main_tz)\n",
    "\n",
    "# get Spark Context UI\n",
    "sc = spark.sparkContext\n",
    "sc # ETH@Review: The link that this prints doesn't work from Eagle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91e3b2",
   "metadata": {},
   "source": [
    "#### The *Spark UI* above works only for local mode. For HPC cluster Spark UI, use:\n",
    "http://localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4214f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b37296",
   "metadata": {},
   "source": [
    "## 3. dsgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 20)\n",
    "import plotly\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from semver import VersionInfo\n",
    "from pydantic import ValidationError\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as sparktypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239217ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsgrid.common import LOCAL_REGISTRY\n",
    "from dsgrid.registry.registry_manager import RegistryManager\n",
    "from dsgrid.utils.files import load_data\n",
    "from dsgrid.utils.spark import create_dataframe, read_dataframe, get_unique_values\n",
    "from dsgrid.dimension.base_models import DimensionType\n",
    "from dsgrid.dataset.dataset import Dataset\n",
    "from dsgrid.project import Project\n",
    "from dsgrid.dimension.time import TimeZone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cec913",
   "metadata": {},
   "source": [
    "## 3.1. Check dsgrid registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce24d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sync registry and then load offline\n",
    "registry_path = os.getenv(\"DSGRID_REGISTRY_PATH\", default=LOCAL_REGISTRY)\n",
    "registry_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_and_pull = True # <--- registry config only\n",
    "if sync_and_pull:\n",
    "    print(f\"syncing registry: {registry_path}\")\n",
    "    RegistryManager.load(registry_path, offline_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETH@Review: Were you intending to write something to the right of the arrow?\n",
    "offline_mode = True # <---\n",
    "\n",
    "registry_mgr = RegistryManager.load(registry_path, offline_mode=offline_mode)\n",
    "project_mgr = registry_mgr.project_manager\n",
    "dataset_mgr = registry_mgr.dataset_manager\n",
    "dim_map_mgr = registry_mgr.dimension_mapping_manager\n",
    "dim_mgr = registry_mgr.dimension_manager\n",
    "# ETH@Review: This line seems out of place. Or change \"Loading\" to \"Loaded\"?\n",
    "print(f\"Loaded dsgrid registry at: {registry_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_mgr.show(max_width=30, drop_fields=[\"Date\", \"Submitter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d7f4c",
   "metadata": {},
   "source": [
    "## 3.2. Load Project\n",
    "This section is mostly exploratory (For *Section 4. Queries*, only need to load project) \n",
    "\n",
    "####  Some user criteria:\n",
    "At the projects, I want to be able to:\n",
    "- Examine what's available in the project:\n",
    "    * Show project dimensions by type, show resolution by type - I don't care: base/supplemental, mappings, id\n",
    "    * Get unique records by dimension/resolution\n",
    "    * Get unique records by selected dimension sets\n",
    "    * Show mapped dataset\n",
    "    * Show unit (or select a unit of analysis) and fuel types\n",
    "- Make queries using:\n",
    "    * Project dimensions + fuel types + time resolutions\n",
    "    * Get all types of statistics (max, mean, min, percentiles, count, sum)\n",
    "    \n",
    "- dataset level: never mapped, think TEMPO,\n",
    "- interface to allow for query optimization\n",
    "    \n",
    "#### Notes:\n",
    " * Project_manager has access to all other managers.\n",
    " * Each manager has the responsiblity to retrieve configs\n",
    " * Access ConfigModel from configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load projct\n",
    "project_id = \"dsgrid_conus_2022\" # <---\n",
    "project = Project.load(project_id, offline_mode=offline_mode)\n",
    "\n",
    "print(\"project loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51fbba",
   "metadata": {},
   "source": [
    "## 3.3. Load Project Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89660b28",
   "metadata": {},
   "source": [
    "### 3.3.3. TEMPO\n",
    "\n",
    "load and check tempo dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13295901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"tempo_conus_2022\" # <----\n",
    "project.load_dataset(dataset_id)\n",
    "tempo = project.get_dataset(dataset_id)\n",
    "print(\"tempo dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE DELETED ###\n",
    "tempo_load_data_lookup = tempo.load_data_lookup\n",
    "\n",
    "file = \"/scratch/dthom/tempo_load_data3.parquet\" # <---\n",
    "tempo_load_data = spark.read.parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_mapped_load_data_lookup = tempo._handler._remap_dimension_columns(tempo_load_data_lookup)\n",
    "tempo_mapped_load_data = tempo._handler._remap_dimension_columns(tempo_load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tempo_load_data_lookup\n",
    "del tempo_load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719ca95",
   "metadata": {},
   "source": [
    "## 4. Queries\n",
    "### Query util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58e461",
   "metadata": {},
   "source": [
    "### 4.1. Hourly electricity consumption by *scenario, model_year, and ReEDS PCA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95324872",
   "metadata": {},
   "outputs": [],
   "source": [
    "### all_enduses-totelectric_enduses map\n",
    "\n",
    "dim_map_id = \"conus-2022-detailed-end-uses-kwh__all-electric-end-uses__c4149547-1209-4ce3-bb4c-3ab292067e8a\" # <---\n",
    "electric_enduses_map = dim_map_mgr.get_by_id(dim_map_id).get_records_dataframe()\n",
    "\n",
    "### get all project electric end uses\n",
    "electric_enduses = electric_enduses_map.filter(\"to_id is not NULL\").select(\"from_id\").toPandas()[\"from_id\"].to_list()\n",
    "electric_enduses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6700c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### county-to-PCA map\n",
    "dim_map_id = \"us_counties_2020_l48__reeds_pca__fcc554e1-87c9-483f-89e3-a0df9563cf89\" # <---\n",
    "county_to_pca_map = dim_map_mgr.get_by_id(dim_map_id).get_records_dataframe()\n",
    "county_to_pca_map.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a76b85",
   "metadata": {},
   "source": [
    "### 4.1.3. TEMPO\n",
    "query TEMPO data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annual_dataframe(df, data_id, year):\n",
    "    dfs = df.filter(f\"id = {data_id}\")\n",
    "    days_per_month = {\n",
    "        1: 31,\n",
    "        2: 28,\n",
    "        3: 31,\n",
    "        4: 30,\n",
    "        5: 31,\n",
    "        6: 30,\n",
    "        7: 31,\n",
    "        8: 31,\n",
    "        9: 30,\n",
    "        10: 31,\n",
    "        11: 30,\n",
    "        12: 31,\n",
    "    }\n",
    "    days_per_year = 366 if year % 4 == 0 else 365 \n",
    "    hours_per_year = days_per_year * 24\n",
    "    td = timedelta(hours=1)\n",
    "    # Set to UTC to avoid DST problems\n",
    "    start_time = datetime(year=year, month=1, day=1, hour=0, tzinfo=pytz.UTC)\n",
    "    # TODO: make columns dynamic\n",
    "    overall = {\n",
    "        \"timestamp\": np.array([start_time + i * td for i in range(hours_per_year)]),\n",
    "        \"L1andL2\": np.empty(hours_per_year, np.float32),\n",
    "        \"DCFC\": np.empty(hours_per_year, np.float32),\n",
    "        \"id\": np.empty(hours_per_year, np.int64),\n",
    "    }\n",
    "        \n",
    "    index = 0\n",
    "    for month in range(1, 13):\n",
    "        df_month = dfs.filter(f\"month = {month}\")\n",
    "        if month == 2 and year % 4 == 0:\n",
    "            num_days = 29\n",
    "        else:\n",
    "            num_days = days_per_month[month]\n",
    "\n",
    "        df_by_day = {}\n",
    "        for day in range(1, num_days + 1):\n",
    "            day_of_week = datetime(year=year, month=month, day=day).weekday()\n",
    "            if day_of_week not in df_by_day:\n",
    "                df_by_day[day_of_week] = df_month.filter(f\"day_of_week = {day_of_week}\").toPandas()\n",
    "                df_by_day[day_of_week].sort_values(by=\"hour\", inplace=True)\n",
    "            end = index + 24\n",
    "            overall[\"L1andL2\"][index:end] = df_by_day[day_of_week][\"L1andL2\"].values\n",
    "            overall[\"L1andL2\"][index:end] = df_by_day[day_of_week][\"L1andL2\"].values\n",
    "            overall[\"DCFC\"][index:end] = df_by_day[day_of_week][\"DCFC\"].values\n",
    "            overall[\"id\"][index:end] = df_by_day[day_of_week][\"id\"].values\n",
    "            index += 24\n",
    "            \n",
    "    assert index == hours_per_year, index\n",
    "    return SparkSession.getActiveSession().createDataFrame(pd.DataFrame(overall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load timezone map (not registered)\n",
    "timezone_file = \"/scratch/lliu2/project_county_timezone/county_fip_to_local_prevailing_time.csv\"\n",
    "tz_map = spark.read.csv(timezone_file, header=True)\n",
    "tz_map = tz_map.withColumn(\"from_fraction\", F.lit(1))\n",
    "tz_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a88b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get electric end uses for transportation\n",
    "tra_elec_enduses = [col for col in tempo_mapped_load_data.columns if col in electric_enduses]\n",
    "tra_elec_enduses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE DELETED\n",
    "# tempo_mapped_load_data_lookup = tempo_mapped_load_data_lookup.filter(\"id in ('1621180393', '770011011', '1058530452')\")\n",
    "# tempo_mapped_load_data = tempo_mapped_load_data.filter(\"id in ('1621180393', '770011011', '1058530452')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54adf133",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 0. consolidate load_data: get total hourly electricity consumption by id\n",
    "# make get_time_cols accessible at dataset level\n",
    "tra_elec_kwh = tempo_mapped_load_data.select(\n",
    "    \"id\",\n",
    "    \"day_of_week\",\n",
    "    \"hour\",\n",
    "    \"month\",\n",
    "    sum([F.col(col) for col in tra_elec_enduses]).alias(\"electricity\")\n",
    ")\n",
    "# tra_elec_kwh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e42de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1. map load_data_lookup to timezone\n",
    "load_data_lookup = tempo_mapped_load_data_lookup.filter(\"id is not NULL\")\\\n",
    ".select(\"sector\", \"scenario\", \"model_year\", \"geography\", \"id\", \"fraction\").join(\n",
    "    tz_map,\n",
    "    on = F.col(\"geography\")==tz_map.from_id,\n",
    "    how = \"left\",\n",
    ").drop(\"from_id\").withColumnRenamed(\"to_id\", \"timezone\")\n",
    "\n",
    "## combine fraction\n",
    "nonfraction_cols = [x for x in load_data_lookup.columns if x not in {\"fraction\", \"from_fraction\"}]\n",
    "load_data_lookup = load_data_lookup.fillna(1, subset=[\"from_fraction\"]).selectExpr(\n",
    "    *nonfraction_cols, \"fraction*from_fraction AS fraction\"\n",
    ")\n",
    "# load_data_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1edc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 2. join load_data and lookup\n",
    "tra_elec_kwh = load_data_lookup.join(\n",
    "    tra_elec_kwh,\n",
    "    on=\"id\",\n",
    "    how=\"left\",\n",
    ").drop(\"id\")\n",
    "\n",
    "tra_elec_kwh = tra_elec_kwh.groupBy(\n",
    "    \"sector\",\n",
    "    \"scenario\", \n",
    "    \"geography\",\n",
    "    \"model_year\",\n",
    "    \"timezone\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    ").agg(F.sum(\n",
    "    F.col(\"fraction\")*F.col(\"electricity\")\n",
    ").alias(\"electricity\")\n",
    "    )\n",
    "\n",
    "## cache df\n",
    "# tra_elec_kwh = tra_elec_kwh.cache()\n",
    "# tra_elec_kwh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9756a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "year = 2012 # <--- weather year\n",
    "sys_tz = TimeZone.EST.tz\n",
    "timezones_local = [TimeZone.EPT, TimeZone.CPT, TimeZone.MPT, TimeZone.PPT]\n",
    "\n",
    "## 3. create range of model_year\n",
    "model_time = []\n",
    "for tz_local in timezones_local:\n",
    "    model_time_df = pd.DataFrame()\n",
    "    # create time range in local time\n",
    "    model_time_df[\"timestamp\"] = pd.date_range(\n",
    "        start=datetime(year=int(year), month=1, day=1, hour=0),\n",
    "        end=datetime(year=int(year), month=12, day=31, hour=23),\n",
    "        tz=tz_local,\n",
    "        freq=\"H\")\n",
    "    model_time_df[\"timezone\"] = tz_local.value\n",
    "    model_time_df[\"day_of_week\"] = model_time_df[\"timestamp\"].dt.day_of_week.astype(str)\n",
    "    model_time_df[\"month\"] = model_time_df[\"timestamp\"].dt.month.astype(str)\n",
    "    model_time_df[\"hour\"] = model_time_df[\"timestamp\"].dt.hour.astype(str)\n",
    "    \n",
    "    # convert to main timezone\n",
    "    model_time_df[\"timestamp\"] = model_time_df[\"timestamp\"].dt.tz_convert(sys_tz)\n",
    "    # wrap time to year\n",
    "    model_time_df[\"timestamp\"] = model_time_df[\"timestamp\"].apply(lambda x: x.replace(year=year))\n",
    "    \n",
    "    model_time.append(model_time_df)\n",
    "    \n",
    "model_time = pd.concat(model_time, axis=0, ignore_index=True)\n",
    "print(model_time)\n",
    "\n",
    "# convert to spark df\n",
    "schema = sparktypes.StructType([\n",
    "    sparktypes.StructField(\"timestamp\", sparktypes.TimestampType(), False), \\\n",
    "    sparktypes.StructField(\"timezone\", sparktypes.StringType(), False), \\\n",
    "    sparktypes.StructField(\"day_of_week\", sparktypes.StringType(), False), \\\n",
    "    sparktypes.StructField(\"month\", sparktypes.StringType(), False), \\\n",
    "    sparktypes.StructField(\"hour\", sparktypes.StringType(), False), \\\n",
    "])\n",
    "model_time = SparkSession.getActiveSession().createDataFrame(model_time, schema=schema)\n",
    "print(model_time.count())\n",
    "model_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 4. expand to model_years\n",
    "tra_elec_kwh = model_time.join(\n",
    "    tra_elec_kwh,\n",
    "    on=[\"timezone\", \"day_of_week\", \"month\", \"hour\"], \n",
    "    how=\"right\"\n",
    ").drop(\"day_of_week\", \"month\", \"hour\")\n",
    "\n",
    "## cache df\n",
    "# tra_elec_kwh = tra_elec_kwh.cache()\n",
    "# tra_elec_kwh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 5. map load_data_lookup to PCA\n",
    "tra_elec_kwh = tra_elec_kwh.join(\n",
    "    county_to_pca_map,\n",
    "    on = F.col(\"geography\")==county_to_pca_map.from_id,\n",
    "    how = \"left\").drop(\"from_id\").drop(\"geography\").withColumnRenamed(\"to_id\", \"geography\").groupBy(\n",
    "    \"sector\",\n",
    "    \"scenario\", \n",
    "    \"geography\",\n",
    "    \"model_year\",\n",
    "    \"timestamp\"\n",
    ").agg(F.sum(\"electricity\").alias(\"electricity\"))\n",
    "\n",
    "# tra_elec_kwh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13233566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### 6. save as partitions\n",
    "tra_output_file = Path(f\"/scratch/{getpass.getuser()}/tempo_projections.parquet\")\n",
    "\n",
    "# # refresh file dir\n",
    "if tra_output_file.exists():\n",
    "    shutil.rmtree(tra_output_file)\n",
    "\n",
    "if tra_output_file.exists():\n",
    "    raise ValueError(f\"file: {tra_output_file} already exist. `shutile.rmtree(tra_output_file)` to override.\")\n",
    "\n",
    "tra_elec_kwh.sort(\"scenario\", \"model_year\", \"geography\", \"timestamp\")\\\n",
    "    .repartition(\"scenario\", \"model_year\").write\\\n",
    "    .partitionBy(\"scenario\", \"model_year\")\\\n",
    "    .option(\"path\", tra_output_file)\\\n",
    "    .saveAsTable(\"tra_elec_kwh\", format='parquet')\n",
    "\n",
    "print(\"tra_elec_kwh saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "########## load transportation projection data ###########\n",
    "tra_output_file = Path(f\"/scratch/{getpass.getuser()}/tempo_projections.parquet\")\n",
    "\n",
    "if tra_output_file.exists():\n",
    "    tra_elec_kwh = read_dataframe(tra_output_file)\n",
    "    print(\"tra_elec_kwh loaded\")\n",
    "else:\n",
    "    print(f\"tra_output_file={tra_output_file} does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
